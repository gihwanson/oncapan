"""
AI ëŒ“ê¸€ ìƒì„± ëª¨ë“ˆ
- OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ“ê¸€ ìƒì„±
- í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ìƒì„± + ëŒ“ê¸€ í’€ fallback
- í’ˆì§ˆ ê²€ì¦ ë° ë°˜ë³µ ë°©ì§€
- í†µê³„ íŒŒì¼ ì €ì¥ ë° ì‹¤íŒ¨ ì›ì¸ ì¶”ì 
"""

import os
import sys
import json
import time
import random
import logging
import re
from enum import Enum
from typing import List, Optional, Dict, Tuple
from datetime import datetime, date
from openai import OpenAI
from openai import APIError, APIConnectionError, RateLimitError

# íŒŒì¼ ë½ ì§€ì›
try:
    if os.name == 'nt':  # Windows
        import msvcrt
    else:  # Unix/Linux
        import fcntl
except ImportError:
    pass

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ValidationFailureReason(Enum):
    """ê²€ì¦ ì‹¤íŒ¨ ì›ì¸"""
    TOO_SHORT = "too_short"
    TOO_LONG = "too_long"
    BANNED_WORD = "banned_word"
    DUPLICATE_RECENT = "duplicate_recent"
    DUPLICATE_POST = "duplicate_post"
    MULTILINE = "multiline"
    SPECIAL_CHAR_SPAM = "special_char_spam"
    BLACKLISTED = "blacklisted"
    EMPTY = "empty"


class AICommentGenerator:
    """AI ëŒ“ê¸€ ìƒì„±ê¸°"""
    
    # ê¸ˆì§€ í‘œí˜„ ëª©ë¡
    FORBIDDEN_PHRASES = [
        'í˜ë‚´ì„¸ìš”', 'í™”ì´íŒ…', 'ì˜ ë  ê±°ì˜ˆìš”', 'ê´œì°®ì•„ì§ˆ ê±°ì˜ˆìš”', 
        'ê¸ì •ì ìœ¼ë¡œ', 'ì‘ì›í•©ë‹ˆë‹¤', 'ì´í•´í•©ë‹ˆë‹¤', 'ê³µê°í•©ë‹ˆë‹¤',
        'ë‹¹ì‹ ì˜', 'ë¶„ëª…íˆ', 'ê²°êµ­', 'ì´ ë˜í•œ ì§€ë‚˜ê°ˆ',
        'í˜ë‚´', 'í™”ì´íŒ…ì…ë‹ˆë‹¤', 'ê±´ìŠ¹', 'ê±´ìŠ¹ì…ë‹ˆë‹¤',
        'í•  ìˆ˜ ìˆì–´', 'ì˜ ë  ê±°ì•¼', 'ê´œì°®ì•„ì§ˆ ê±°ì•¼',
        # ì„¤ëª…ì /ê°íƒ„ì  í‘œí˜„
        'ì§„ì§œ', 'ë„ˆë¬´', 'ì°¸', 'ì •ë§', 'ëŒ€ë‹¨', 'ì™€', 'ì•„',
        # ë°˜ë§ íŒ¨í„´
        '~ì•¼', '~ì§€', '~ë„¤', '~ì–´', '~ì•„'
    ]
    
    # API ì œí•œ ì„¤ì •
    DAILY_API_CALL_LIMIT = 500  # ì¼ì¼ API í˜¸ì¶œ ìƒí•œ
    DAILY_TOKEN_LIMIT = 200000  # ì¼ì¼ í† í° ìƒí•œ (200k tokens)
    
    # í†µê³„ ì €ì¥ ê°„ê²© (ì´ˆ)
    STATS_SAVE_INTERVAL = 60  # 1ë¶„ë§ˆë‹¤ ì €ì¥
    
    def __init__(self, api_key: str, learning_analyzer=None, 
                 prompt_version: str = "v2", 
                 max_history: int = 50):
        """
        Args:
            api_key: OpenAI API í‚¤
            learning_analyzer: LearningAnalyzer ì¸ìŠ¤í„´ìŠ¤ (ì„ íƒ)
            prompt_version: í”„ë¡¬í”„íŠ¸ ë²„ì „ (ê¸°ë³¸: v1)
            max_history: ë°˜ë³µ ë°©ì§€ë¥¼ ìœ„í•œ ìµœê·¼ ëŒ“ê¸€ íˆìŠ¤í† ë¦¬ í¬ê¸°
        """
        self.client = OpenAI(api_key=api_key)
        self.learning_analyzer = learning_analyzer
        self.prompt_version = prompt_version
        self.max_history = max_history
        self.hot_reload_interval = 300
        self.last_pool_reload = time.time()
        
        # íŒŒì¼ ê²½ë¡œ ì„¤ì •
        self._init_file_paths()
        
        # ë°˜ë³µ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ“ê¸€ íˆìŠ¤í† ë¦¬ (ì „ì—­)
        self.comment_history: List[str] = []
        
        # ê²Œì‹œê¸€ë³„ ëŒ“ê¸€ íˆìŠ¤í† ë¦¬ (ê°™ì€ ê²Œì‹œê¸€ì— ê°™ì€ ëŒ“ê¸€ ë°©ì§€)
        self.post_comment_map: Dict[str, str] = {}  # post_id -> comment
        
        # ëŒ“ê¸€ í’€ ë° ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë¡œë“œ
        self.comment_pool: Dict[str, List[str]] = {}
        self.blacklist: set = set()
        self._load_comment_pool()
        
        # í”„ë¡¬í”„íŠ¸ ë¡œë“œ
        self.system_prompt = self._load_prompt(prompt_version)
        
        # í†µê³„ ë¡œë“œ (ì¬ì‹œì‘ í›„ì—ë„ ëˆ„ì )
        self.stats = self._load_stats()
        
        # API ì‚¬ìš©ëŸ‰ ì¶”ì  (ì¼ì¼ ë¦¬ì…‹)
        self.api_usage = self._load_api_usage()
        self._check_daily_reset()
        
        # ì‹¤íŒ¨ ì›ì¸ ì¹´ìš´í„°
        self.failure_reasons: Dict[str, int] = {
            reason.value: 0 for reason in ValidationFailureReason
        }
        
        # í’€ ëª¨ë“œ ê°•ì œ ì—¬ë¶€ (API ì œí•œ ë„ë‹¬ ì‹œ)
        self.force_pool_mode = False
        
        # í†µê³„ ì €ì¥ ê´€ë ¨
        self.last_stats_save = time.time()
        self.stats_dirty = False  # í†µê³„ ë³€ê²½ ì—¬ë¶€
        
        # ì¢‹ì•„ìš” ë°ì´í„° ë¡œë“œ
        self.likes: Dict[str, bool] = {}  # post_id -> True (ì¢‹ì•„ìš” ëˆ„ë¦„)
        self._load_likes()
        
        logger.info(f"AICommentGenerator ì´ˆê¸°í™” ì™„ë£Œ (í”„ë¡¬í”„íŠ¸: {prompt_version}, í’€: {len(self.comment_pool)}ê°œ)")
    
    def _init_file_paths(self):
        """íŒŒì¼ ê²½ë¡œ ì´ˆê¸°í™”"""
        if getattr(sys, 'frozen', False):
            base_path = os.path.dirname(sys.executable)
        else:
            base_path = os.path.dirname(os.path.abspath(__file__))
        
        self.stats_file = os.path.join(base_path, "stats.json")
        self.comment_pool_file = os.path.join(base_path, "comment_pool.json")
        self.prompts_dir = os.path.join(base_path, "prompts")
        self.likes_file = os.path.join(base_path, "likes.json")
    
    def _load_comment_pool(self):
        """ëŒ“ê¸€ í’€ íŒŒì¼ ë¡œë“œ (íŒŒì¼ ë½ ì‚¬ìš©)"""
        try:
            if os.path.exists(self.comment_pool_file):
                with open(self.comment_pool_file, 'r', encoding='utf-8') as f:
                    # íŒŒì¼ ë½
                    try:
                        if os.name == 'nt':
                            try:
                                msvcrt.locking(f.fileno(), msvcrt.LK_LOCK, 1)
                            except NameError:
                                pass
                        else:
                            try:
                                fcntl.flock(f.fileno(), fcntl.LOCK_SH)
                            except NameError:
                                pass
                    except:
                        pass
                    
                    data = json.load(f)
                    
                    # ë½ í•´ì œ
                    try:
                        if os.name == 'nt':
                            try:
                                msvcrt.locking(f.fileno(), msvcrt.LK_UNLCK, 1)
                            except NameError:
                                pass
                        else:
                            try:
                                fcntl.flock(f.fileno(), fcntl.LOCK_UN)
                            except NameError:
                                pass
                    except:
                        pass
                    
                    # ê¸°ì¡´ í˜•ì‹ í˜¸í™˜ì„± ìœ ì§€
                    old_comments = data.get('comments', [])
                    if old_comments and isinstance(old_comments, list):
                        # ê¸°ì¡´ í˜•ì‹: ë‹¨ì¼ ë¦¬ìŠ¤íŠ¸ -> ì¼ë°˜ ì¹´í…Œê³ ë¦¬ë¡œ ë³€í™˜
                        self.comment_pool = {
                            'ì¼ë°˜': old_comments,
                            **self._get_default_pool()
                        }
                        # ì¼ë°˜ ì¹´í…Œê³ ë¦¬ì—ì„œ ì¤‘ë³µ ì œê±°
                        for key in self.comment_pool:
                            if key != 'ì¼ë°˜':
                                self.comment_pool[key] = [c for c in self.comment_pool[key] if c not in old_comments]
                    else:
                        # ìƒˆ í˜•ì‹: ìœ í˜•ë³„ ë”•ì…”ë„ˆë¦¬
                        self.comment_pool = data.get('comment_pools', self._get_default_pool())
                        if not isinstance(self.comment_pool, dict):
                            self.comment_pool = self._get_default_pool()
                    
                    self.blacklist = set(data.get('blacklist', []))
                    total_comments = sum(len(pool) for pool in self.comment_pool.values())
                    logger.info(f"ëŒ“ê¸€ í’€ ë¡œë“œ ì™„ë£Œ: {total_comments}ê°œ ëŒ“ê¸€ ({len(self.comment_pool)}ê°œ ìœ í˜•), {len(self.blacklist)}ê°œ ë¸”ë™ë¦¬ìŠ¤íŠ¸")
            else:
                # ê¸°ë³¸ ëŒ“ê¸€ í’€ ì‚¬ìš©
                self.comment_pool = self._get_default_pool()
                self._save_comment_pool()
                total_comments = sum(len(pool) for pool in self.comment_pool.values())
                logger.info(f"ê¸°ë³¸ ëŒ“ê¸€ í’€ ìƒì„± ì™„ë£Œ: {total_comments}ê°œ ëŒ“ê¸€ ({len(self.comment_pool)}ê°œ ìœ í˜•)")
        except Exception as e:
            logger.error(f"ëŒ“ê¸€ í’€ ë¡œë“œ ì˜¤ë¥˜: {e}")
            self.comment_pool = self._get_default_pool()
            self.blacklist = set()
    
    def _get_default_pool(self) -> Dict[str, List[str]]:
        """ê²Œì‹œê¸€ ìœ í˜•ë³„ ê¸°ë³¸ ëŒ“ê¸€ í’€"""
        return {
            'ê±°ë˜': [
                'ì¿¨ê±°í•˜ì„¸ì—¬', 'ì¿¨ê±°í•˜ì„¸ìš”', 'ì¿¨ê±°í•˜ì„¸ì˜', 'ì¿¨ê±°ì—¬', 'ì¿¨ê±°ì—¬ ã…',
                'ì¡´ê±°ë˜í•˜ì„¸ì˜', 'ì¢‹ì€ê±°ë˜í•˜ì„¸ìš”', 'ê±°ë˜ ì˜ í•˜ì„¸ìš©', 'ì¿¨ê±°ë˜ í•˜ì„¸ìš”',
                'ì¿¨ê±° í•˜ì‹œê¸¸', 'ì¿¨ê±° ê³ ê³ ', 'ë¬´ì‚¬ê±°ë˜ìš”', 'ê¹”ë”ê±°ë˜ìš”',
                'ì¿¨ê±°í•˜ì…”ìš”', 'ì¿¨ê±°í•˜ì„¸ìš©', 'ì¿¨ê±°í•˜ì‹œê¸¸', 'ì¿¨ê±°í•˜ì„¸ì—¬ ã…',
                'ì¡´ê±°ë˜ìš”', 'ì¢‹ì€ê±°ë˜ìš”', 'ê±°ë˜ ì˜ í•˜ì„¸ìš”', 'ì¿¨ê±°ë˜ìš”'
            ],
            'ëŒë°œ': [
                'ë¬´ì‚¬ê·€í™˜í•©ì‹œë‹¹', 'ë¬´ì‚¬ê·€í™˜ë ', 'ë¬´ì‚¬ê·€í™˜ê°€ì—¬', 'ê±´ìŠ¹í•´ìš”',
                'ë¬´ì‚¬ê·€í™˜ í•˜ìêµ¬ì—¬', 'ëŒë°œ ë¬´ê·€ì…ë‹ˆë‹¤', 'ë¬´ì¶œê¸°ì›í•©ë‹ˆë‹¤',
                'ë¬´ì‚¬íˆ ê·€í™˜í•´ìš”', 'ë¬´ì‚¬ê·€í™˜ í•©ì‹œë‹¹~', 'ë¬´ì‚¬ê·€í™˜ìš”',
                'ìœ„ì¦ˆ ë¬´ì‚¬ê·€í™˜ìš”', 'ìœ„ì¦ˆ ë¬´ê·€ ê°€ì—¬', 'ìœ„ì¦ˆ ë¬´ì‚¬ê·€í™˜í•©ì‹œë‹¤',
                'ëŒë°œ ë¬´ì¶œ ê¸°ì›', 'ëŒë°œ ë¬´ì‚¬ê·€í™˜ìš”', 'ëŒë°œ ë¬´ê·€ ê°€ì¦ˆì•„',
                'ë¬´ê·€ ê¸°ì›í•©ë‹ˆë‹¹', 'ìœ„ì¦ˆ ëŒë°œì´ë„¤ì˜', 'ëŒë°œ ë¬´ì‚¬ê·€í™˜ ê°€ì—¬',
                'ë¬´ì¶œ ê¸°ì›í•©ë‹ˆë‹¤', 'ë¬´ì‚¬ê·€í™˜ ê°€ìš”', 'ë¬´ê·€ ê¸°ì›í•©ë‹ˆë‹¹'
            ],
            'í›„ê¸°': [
                'ì¢‹ì€ í›„ê¸°ë„¤ìš”', 'í›„ê¸° ê°ì‚¬í•´ìš”', 'ë„ì›€ëì–´ìš”', 'ì°¸ê³ í•˜ê² ìŠµë‹ˆë‹¤',
                'ì¢‹ì€ ì •ë³´ë„¤ìš”', 'ìœ ìš©í•˜ë„¤ìš”', 'ê°ì‚¬í•©ë‹ˆë‹¤', 'ë„ì›€ëìŠµë‹ˆë‹¤',
                'ì¢‹ë„¤ìš”', 'ê´œì°®ë„¤ìš”', 'ê´œì°®ì•„ìš”', 'ì¢‹ì•„ìš”', 'ì¢‹ìŠµë‹ˆë‹¤'
            ],
            'ë©˜íƒˆ': [
                'ê·¸ëŸ¬ê²Œìš”', 'ì‰½ì§€ ì•Šë„¤ìš”', 'ë³µì¡í•˜ë„¤ìš”', 'ë¬´ë‚œí•˜ë„¤ìš”',
                'ê·¸ë ‡ë„¤ìš”', 'ë§ë„¤ìš”', 'ê·¸ëŸ°ê°€ìš”', 'ê·¸ë ‡êµ°ìš”', 'ê·¸ë ‡ì£ ',
                'ë§ì•„ìš”', 'ê·¸ë˜ìš”', 'ê·¸ë ‡ìŠµë‹ˆë‹¤', 'ë§ìŠµë‹ˆë‹¤'
            ],
            'ì¼ë°˜': [
                'ê·¸ëŸ¬ê²Œìš”', 'ì• ë§¤í•˜ë„¤ìš”', 'ì‰½ì§€ ì•Šë„¤ìš”', 'ë³µì¡í•˜ë„¤ìš”', 'ë¬´ë‚œí•˜ë„¤ìš”',
                'ë¹„ìŠ·í•©ë‹ˆë‹¤', 'ê·¸ëŸ´ë“¯í•˜ë„¤ìš”', 'ì¶•í•˜í•©ë‹ˆë‹¤', 'ê·¸ë ‡ë„¤ìš”', 'ë§ë„¤ìš”',
                'ê·¸ëŸ°ê°€ìš”', 'ê·¸ë ‡êµ°ìš”', 'ê·¸ë ‡ì£ ', 'ë§ì•„ìš”', 'ê·¸ë˜ìš”',
                'ê·¸ë ‡ìŠµë‹ˆë‹¤', 'ë§ìŠµë‹ˆë‹¤', 'ê·¸ë ‡ë„¤', 'ë§ë„¤', 'ê·¸ë˜'
            ],
            'ê±´ìŠ¹': [
                'ê±´ìŠ¹í•˜ì„¸ìš”', 'ê±´ìŠ¹ì…ë‹ˆë‹¤', 'ê±´ìŠ¹í•©ì‹œë‹¤', 'ê±´ìŠ¹ì´ìš”', 'ê±´ìŠ¹í•´ìš”',
                'ê±´ìŠ¹í•˜ì‹œê¸¸', 'ê±´ìŠ¹í•˜ì„¸ì˜', 'ê±´ìŠ¹í•˜ì„¸ì—¬', 'ê±´ìŠ¹ì´ë„¤ìš”', 'ê±´ìŠ¹ì´ì—ì˜',
                'ê±´ìŠ¹ì—ì—ì˜', 'ê±´ìŠ¹ì´ì—°', 'ê±´ìŠ¹í•˜ì‹œê¸¸ìš”', 'ê±´ìŠ¹í•˜ì„¸ìš©', 'ê±´ìŠ¹í•˜ì…”ìš”',
                'ê±´ìŠ¹ì´ìš”~', 'ê±´ìŠ¹í•©ë‹ˆë‹¹', 'ê±´ìŠ¹í•˜ìêµ¬ì—¬', 'ê±´ìŠ¹ì´ë„¤ì˜', 'ê±´ìŠ¹í•´ìš”~'
            ]
        }
    
    def _save_comment_pool(self):
        """ëŒ“ê¸€ í’€ íŒŒì¼ ì €ì¥ (íŒŒì¼ ë½ ì‚¬ìš©)"""
        try:
            data = {
                'comment_pools': self.comment_pool,  # ìœ í˜•ë³„ í’€
                'blacklist': list(self.blacklist),
                'meta': {
                    'version': '2.0',  # ë²„ì „ ì—…ë°ì´íŠ¸
                    'last_updated': datetime.now().isoformat()
                }
            }
            temp_file = self.comment_pool_file + '.tmp'
            with open(temp_file, 'w', encoding='utf-8') as f:
                # íŒŒì¼ ë½
                try:
                    if os.name == 'nt':
                        try:
                            msvcrt.locking(f.fileno(), msvcrt.LK_LOCK, 1)
                        except NameError:
                            pass
                    else:
                        try:
                            fcntl.flock(f.fileno(), fcntl.LOCK_EX)
                        except NameError:
                            pass
                except:
                    pass
                
                json.dump(data, f, ensure_ascii=False, indent=2)
                f.flush()
                os.fsync(f.fileno())
                
                # ë½ í•´ì œ
                try:
                    if os.name == 'nt':
                        try:
                            msvcrt.locking(f.fileno(), msvcrt.LK_UNLCK, 1)
                        except NameError:
                            pass
                    else:
                        try:
                            fcntl.flock(f.fileno(), fcntl.LOCK_UN)
                        except NameError:
                            pass
                except:
                    pass
            
            # ì›ìì  ì´ë™
            if os.path.exists(self.comment_pool_file):
                os.replace(temp_file, self.comment_pool_file)
            else:
                os.rename(temp_file, self.comment_pool_file)
            
            logger.debug("ëŒ“ê¸€ í’€ ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ëŒ“ê¸€ í’€ ì €ì¥ ì˜¤ë¥˜: {e}")
            try:
                if os.path.exists(temp_file):
                    os.remove(temp_file)
            except:
                pass
    
    def reload_comment_pool(self):
        """ëŒ“ê¸€ í’€ í•«ë¦¬ë¡œë“œ (ì‹¤í–‰ ì¤‘ íŒŒì¼ ë³€ê²½ ë°˜ì˜)"""
        self._load_comment_pool()
        logger.info("ëŒ“ê¸€ í’€ í•«ë¦¬ë¡œë“œ ì™„ë£Œ")
    
    def _load_stats(self) -> Dict:
        """í†µê³„ íŒŒì¼ ë¡œë“œ (ì¬ì‹œì‘ í›„ì—ë„ ëˆ„ì , íŒŒì¼ ë½ ì‚¬ìš©)"""
        try:
            if os.path.exists(self.stats_file):
                with open(self.stats_file, 'r', encoding='utf-8') as f:
                    # íŒŒì¼ ë½
                    try:
                        if os.name == 'nt':
                            try:
                                msvcrt.locking(f.fileno(), msvcrt.LK_LOCK, 1)
                            except NameError:
                                pass
                        else:
                            try:
                                fcntl.flock(f.fileno(), fcntl.LOCK_SH)
                            except NameError:
                                pass
                    except:
                        pass
                    
                    stats = json.load(f)
                    
                    # ë½ í•´ì œ
                    try:
                        if os.name == 'nt':
                            try:
                                msvcrt.locking(f.fileno(), msvcrt.LK_UNLCK, 1)
                            except NameError:
                                pass
                        else:
                            try:
                                fcntl.flock(f.fileno(), fcntl.LOCK_UN)
                            except NameError:
                                pass
                    except:
                        pass
                    
                    # ëˆ„ì  í†µê³„ ìœ ì§€
                    return {
                        'generated_total': stats.get('generated_total', 0),
                        'gpt_used': stats.get('gpt_used', 0),  # í•˜ìœ„ í˜¸í™˜ì„±
                        'classification_used': stats.get('classification_used', 0),  # ê²Œì‹œê¸€ ë¶„ë¥˜ ì‚¬ìš© íšŸìˆ˜
                        'pool_used': stats.get('pool_used', 0),
                        'skipped': stats.get('skipped', 0),
                        'validation_fail_total': stats.get('validation_fail_total', 0),
                        'regen_count': stats.get('regen_count', 0),
                        'api_errors': stats.get('api_errors', 0),
                        'last_updated': stats.get('last_updated', datetime.now().isoformat()),
                        'failure_reasons': stats.get('failure_reasons', {})
                    }
            else:
                return self._init_stats()
        except Exception as e:
            logger.error(f"í†µê³„ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return self._init_stats()
    
    def _init_stats(self) -> Dict:
        """ì´ˆê¸° í†µê³„ êµ¬ì¡°"""
        return {
            'generated_total': 0,
            'gpt_used': 0,  # í•˜ìœ„ í˜¸í™˜ì„±
            'classification_used': 0,  # ê²Œì‹œê¸€ ë¶„ë¥˜ ì‚¬ìš© íšŸìˆ˜
            'pool_used': 0,
            'skipped': 0,
            'validation_fail_total': 0,
            'regen_count': 0,
            'api_errors': 0,
            'last_updated': datetime.now().isoformat(),
            'failure_reasons': {}
        }
    
    def _save_stats(self, force: bool = False):
        """í†µê³„ íŒŒì¼ ì €ì¥ (ë°°ì¹˜ ì €ì¥, íŒŒì¼ ë½ ì‚¬ìš©)"""
        current_time = time.time()
        
        # ê°•ì œ ì €ì¥ì´ ì•„ë‹ˆê³ , ê°„ê²©ì´ ì•ˆ ì§€ë‚¬ê³ , ë³€ê²½ì‚¬í•­ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ
        if not force and (current_time - self.last_stats_save < self.STATS_SAVE_INTERVAL) and not self.stats_dirty:
            return
        
        try:
            self.stats['last_updated'] = datetime.now().isoformat()
            self.stats['failure_reasons'] = self.failure_reasons.copy()
            self.stats['api_usage'] = self.api_usage.copy()
            
            temp_file = self.stats_file + '.tmp'
            with open(temp_file, 'w', encoding='utf-8') as f:
                # íŒŒì¼ ë½
                try:
                    if os.name == 'nt':
                        try:
                            msvcrt.locking(f.fileno(), msvcrt.LK_LOCK, 1)
                        except NameError:
                            pass
                    else:
                        try:
                            fcntl.flock(f.fileno(), fcntl.LOCK_EX)
                        except NameError:
                            pass
                except:
                    pass
                
                json.dump(self.stats, f, ensure_ascii=False, indent=2)
                f.flush()
                os.fsync(f.fileno())
                
                # ë½ í•´ì œ
                try:
                    if os.name == 'nt':
                        try:
                            msvcrt.locking(f.fileno(), msvcrt.LK_UNLCK, 1)
                        except NameError:
                            pass
                    else:
                        try:
                            fcntl.flock(f.fileno(), fcntl.LOCK_UN)
                        except NameError:
                            pass
                except:
                    pass
            
            # ì›ìì  ì´ë™
            if os.path.exists(self.stats_file):
                os.replace(temp_file, self.stats_file)
            else:
                os.rename(temp_file, self.stats_file)
            
            self.last_stats_save = current_time
            self.stats_dirty = False
        except Exception as e:
            logger.error(f"í†µê³„ ì €ì¥ ì˜¤ë¥˜: {e}")
            try:
                if os.path.exists(temp_file):
                    os.remove(temp_file)
            except:
                pass
    
    def _load_api_usage(self) -> Dict:
        """API ì‚¬ìš©ëŸ‰ ë¡œë“œ"""
        try:
            if os.path.exists(self.stats_file):
                with open(self.stats_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    usage = data.get('api_usage', {})
                    return {
                        'calls_today': usage.get('calls_today', 0),
                        'tokens_today': usage.get('tokens_today', 0),
                        'last_reset_date': usage.get('last_reset_date', date.today().isoformat())
                    }
            return {
                'calls_today': 0,
                'tokens_today': 0,
                'last_reset_date': date.today().isoformat()
            }
        except Exception as e:
            logger.error(f"API ì‚¬ìš©ëŸ‰ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return {
                'calls_today': 0,
                'tokens_today': 0,
                'last_reset_date': date.today().isoformat()
            }
    
    def _check_daily_reset(self):
        """ì¼ì¼ ë¦¬ì…‹ í™•ì¸"""
        today = date.today().isoformat()
        if self.api_usage['last_reset_date'] != today:
            self.api_usage['calls_today'] = 0
            self.api_usage['tokens_today'] = 0
            self.api_usage['last_reset_date'] = today
            self.force_pool_mode = False
            logger.info("ì¼ì¼ API ì‚¬ìš©ëŸ‰ ë¦¬ì…‹")
    
    def _check_api_limits(self) -> bool:
        """API ì œí•œ í™•ì¸ (True: ì œí•œ ë„ë‹¬, False: ì‚¬ìš© ê°€ëŠ¥)"""
        self._check_daily_reset()
        
        if (self.api_usage['calls_today'] >= self.DAILY_API_CALL_LIMIT or
            self.api_usage['tokens_today'] >= self.DAILY_TOKEN_LIMIT):
            if not self.force_pool_mode:
                logger.warning(f"API ì œí•œ ë„ë‹¬: í˜¸ì¶œ {self.api_usage['calls_today']}/{self.DAILY_API_CALL_LIMIT}, "
                             f"í† í° {self.api_usage['tokens_today']}/{self.DAILY_TOKEN_LIMIT}")
                self.force_pool_mode = True
            return True
        return False
    
    def _load_prompt(self, version: str) -> str:
        """í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¡œë“œ"""
        try:
            prompt_file = os.path.join(self.prompts_dir, f"comment_style_{version}.txt")
            
            if os.path.exists(prompt_file):
                with open(prompt_file, 'r', encoding='utf-8') as f:
                    prompt = f.read().strip()
                logger.info(f"í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì„±ê³µ: {prompt_file}")
                return prompt
            else:
                logger.warning(f"í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {prompt_file}")
                return self._get_default_prompt()
        except Exception as e:
            logger.error(f"í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return self._get_default_prompt()
    
    def _get_default_prompt(self) -> str:
        """ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ (ëŒ“ê¸€ í›„ë³´ ìƒì„±ìš©)"""
        return """ì—­í• 
ë„ˆëŠ” ì˜¨ë¼ì¸ ì»¤ë®¤ë‹ˆí‹°ì—ì„œ í”íˆ ë³´ì´ëŠ” ì§§ê³  ë¬´ë‚œí•œ ë°˜ì‘ ëŒ“ê¸€ ì´ˆì•ˆ ìƒì„±ê¸°ë‹¤.
í† ë¡ Â·ì¡°ì–¸Â·ë¶„ì„ì„ í•˜ì§€ ì•ŠëŠ”ë‹¤.

ì…ë ¥

ê²Œì‹œê¸€ ì œëª©

ê²Œì‹œê¸€ ë³¸ë¬¸

ì´ë¯¸ ë‹¬ë¦° ëŒ“ê¸€ ëª‡ ê°œ

ì¶œë ¥ ëª©í‘œ
ì»¤ë®¤ë‹ˆí‹° ë¶„ìœ„ê¸°ì— ë¬»íˆëŠ” ì§§ì€ ë°˜ì‘í˜• ëŒ“ê¸€ í›„ë³´ë¥¼ ë§Œë“ ë‹¤.

ê·œì¹™

ëŒ“ê¸€ì€ í•œ ì¤„, 6~14ì ìœ„ì£¼ë¡œ ì‘ì„±

ë¬¸ì¥ ì™„ì„±ë„ë¥¼ ì¼ë¶€ëŸ¬ ë‚®ì¶°ë¼ (êµ¬ì–´ì²´, ì¶•ì•½ í—ˆìš©)

ì¡°ì–¸, íŒë‹¨, í•´ê²°ì±…, ì„¤ëª… ê¸ˆì§€

ê°íƒ„Â·ë™ì¡°Â·ê³µê° ì¤‘ í•˜ë‚˜ë§Œ ë‹´ì•„ë¼

ì´ëª¨ì§€ ê¸ˆì§€, ëŠë‚Œí‘œëŠ” ìµœëŒ€ 1ê°œ

ì´ë¯¸ ë‹¬ë¦° ëŒ“ê¸€ê³¼ ì˜ë¯¸Â·ì–´ì¡°ê°€ ê²¹ì³ë„ ë˜ì§€ë§Œ ë¬¸ì¥ì€ ë‹¬ë¼ì•¼ í•œë‹¤

"ê¹”ë”í•¨/ì •ì¤‘í•¨/ì •ë³´ì„±"ì´ ëŠê»´ì§€ë©´ íƒˆë½ì´ë‹¤

ì‘ì—… ì ˆì°¨

(1) ì´ ê²Œì‹œê¸€ì„ ì•„ë˜ ìœ í˜• ì¤‘ í•˜ë‚˜ë¡œë§Œ ë¶„ë¥˜í•œë‹¤
ì¼ìƒ/ìˆ˜ë‹¤ Â· ê°ì •í† ë¡œ Â· ê±°ë˜ Â· ëŒë°œ/ëŒ€ê¸° Â· ê²°ê³¼í›„ê¸° Â· ê°íƒ„/ìë‘

(2) í•´ë‹¹ ìœ í˜•ì—ì„œ ì‚¬ëŒë“¤ì´ í”íˆ ì“°ëŠ” ë°˜ì‘ íŒ¨í„´ì„ ë– ì˜¬ë¦°ë‹¤

(3) ê·¸ íŒ¨í„´ ì•ˆì—ì„œ íŠ€ì§€ ì•ŠëŠ” ëŒ“ê¸€ í›„ë³´ 8ê°œë¥¼ ë§Œë“ ë‹¤

ì¶œë ¥ í˜•ì‹

í›„ë³´ ëŒ“ê¸€ë§Œ ì¤„ë°”ê¿ˆìœ¼ë¡œ ì¶œë ¥

ì„¤ëª…, ë¶„ë¥˜ ê²°ê³¼, ì½”ë©˜íŠ¸ ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ ê²ƒ"""
    
    def can_generate_comment(self, post_content: str) -> bool:
        """ëŒ“ê¸€ ìƒì„± ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        if not post_content or len(post_content.strip()) < 3:
            return False
        return True
    
    def _extract_keywords(self, comments: List[str] = None, post_title: str = "", post_content: str = "") -> List[str]:
        """ëŒ“ê¸€, ì œëª©, ë³¸ë¬¸ì—ì„œ ì˜ë¯¸ ìˆëŠ” í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []
        
        # ì¤‘ìš” í‚¤ì›Œë“œ ìš°ì„  ê²€ìƒ‰ (ê²Œì‹œê¸€ ì œëª©/ë³¸ë¬¸ì—ì„œ)
        important_keywords = [
            'ê±´ìŠ¹', 'ì¿¨ê±°', 'ë¬´ì‚¬ê·€í™˜', 'ë¬´ê·€', 'ë¬´ì¶œ', 'ì¡´ê±°ë˜', 'ëŒë°œ', 'ìœ„ì¦ˆ', 
            'ë±…', 'ì¥ì¤„', 'í¬ì¸íŠ¸', 'ì½©', 'ì‚½ë‹ˆë‹¤', 'íŒë‹ˆë‹¤', 'ê±°ë˜', 'êµ¬ë§¤', 'íŒë§¤',
            'í›„ê¸°', 'ì‹ ê²œ', 'í•´ë´„', 'ê²°ê³¼', 'ë°°ì†¡', 'ì™„ë£Œ', 'ë„ì°©',
            'ë©˜íƒˆ', 'í•˜ì•„', 'ë§ˆë µ', 'í˜ë“œ', 'ì–´ë µ', 'ìŠ¤íŠ¸ë ˆìŠ¤', 'ê³ ë¯¼', 'í˜ë“¤'
        ]
        
        combined_text = (post_title + " " + post_content).lower()
        for keyword in important_keywords:
            if keyword in combined_text:
                keywords.append(keyword)
        
        # ëŒ“ê¸€ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        if comments:
            # ì¡°ì‚¬ ëª©ë¡ (ì œì™¸í•  ë‹¨ì–´ë“¤)
            particles = [
                'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ì™€', 'ê³¼', 'ë„', 'ë§Œ', 'ì¡°ì°¨', 'ê¹Œì§€',
                'ì—ì„œ', 'ì—ê²Œ', 'ê»˜ì„œ', 'í•œí…Œ', 'ë”ëŸ¬', 'ë¡œ', 'ìœ¼ë¡œ', 'ì²˜ëŸ¼', 'ê°™ì´',
                'ë§Œí¼', 'ë³´ë‹¤', 'ë¶€í„°', 'ê¹Œì§€', 'ì¡°ì°¨', 'ë§ˆì €', 'ì€', 'ëŠ”', 'ë„',
                'ë¼ë„', 'ì´ë¼ë„', 'ì´ë‚˜', 'ì´ë‚˜ë§ˆ', 'ë“ ì§€', 'ë“ ê°€', 'ë“ ', 'ì¡°ì°¨',
                'ìš”', 'ì˜', 'ì—¬', 'ì„¸ì˜', 'ì„¸ìš”', 'ì„¸ìš”', 'í•˜ì„¸ìš”', 'í•˜ì„¸ì˜'
            ]
            
            # ëŒ“ê¸€ì—ì„œ ì˜ë¯¸ ìˆëŠ” ë‹¨ì–´ ì¶”ì¶œ
            for comment in comments[:10]:
                if not comment or len(comment.strip()) < 2:
                    continue
                
                # ì¤‘ìš” í‚¤ì›Œë“œê°€ ëŒ“ê¸€ì— ìˆëŠ”ì§€ í™•ì¸
                for keyword in important_keywords:
                    if keyword in comment and keyword not in keywords:
                        keywords.append(keyword)
                
                # 2-5ì í•œê¸€ ë‹¨ì–´ ì¶”ì¶œ (ì¡°ì‚¬ ì œì™¸)
                words = re.findall(r'[ê°€-í£]{2,5}', comment)
                for word in words:
                    # ì¡°ì‚¬ê°€ ì•„ë‹ˆê³ , ì¤‘ìš” í‚¤ì›Œë“œê°€ ì•„ë‹ˆë©°, ì˜ë¯¸ ìˆëŠ” ë‹¨ì–´ì¸ ê²½ìš°
                    if (word not in particles and 
                        word not in keywords and 
                        len(word) >= 2 and
                        word not in ['ê²Œì‹œ', 'ëŒ“ê¸€', 'ì‘ì„±', 'ì¡°íšŒ', 'ì¶”ì²œ', 'ë¹„ì¶”', 'ëª©ë¡', 'ì´ì „', 'ë‹¤ìŒ']):
                        keywords.append(word)
        
        # ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ 10ê°œ ë°˜í™˜
        unique_keywords = []
        seen = set()
        for kw in keywords:
            if kw not in seen:
                unique_keywords.append(kw)
                seen.add(kw)
                if len(unique_keywords) >= 10:
                    break
        
        return unique_keywords
    
    def _detect_post_type_heuristic(self, post_content: str, post_title: str = "") -> str:
        """íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ê²Œì‹œê¸€ ìœ í˜• íŒë‹¨ (fallbackìš©)"""
        combined_text = (post_title + " " + post_content).lower()
        
        # ê±°ë˜ ê´€ë ¨ í‚¤ì›Œë“œ
        trade_keywords = ['ì‚½ë‹ˆë‹¤', 'íŒë‹ˆë‹¤', 'ì¿¨ê±°', 'í¬ì¸íŠ¸', 'ì½©', 'ê±°ë˜', 'êµ¬ë§¤', 'íŒë§¤', 'ì¡´ê±°ë˜']
        if any(keyword in combined_text for keyword in trade_keywords):
            return 'ê±°ë˜'
        
        # ëŒë°œ/ëŒ€ê¸° ê´€ë ¨ í‚¤ì›Œë“œ
        event_keywords = ['ëŒë°œ', 'ëŒ€ê¸°', 'ë¬´ì‚¬ê·€í™˜', 'ë¬´ì¶œ', 'ìœ„ì¦ˆ', 'ë±…', 'ì¥ì¤„']
        if any(keyword in combined_text for keyword in event_keywords):
            return 'ëŒë°œ'
        
        # í›„ê¸° ê´€ë ¨ í‚¤ì›Œë“œ
        review_keywords = ['í›„ê¸°', 'ì‹ ê²œ', 'í•´ë´„', 'ê²°ê³¼', 'ë°°ì†¡', 'ì™„ë£Œ', 'ë„ì°©']
        if any(keyword in combined_text for keyword in review_keywords):
            return 'í›„ê¸°'
        
        # ë©˜íƒˆ ê´€ë ¨ í‚¤ì›Œë“œ
        mental_keywords = ['í•˜ì•„', 'ë§ˆë µ', 'ë©˜íƒˆ', 'í˜ë“œ', 'ì–´ë µ', 'ìŠ¤íŠ¸ë ˆìŠ¤', 'ê³ ë¯¼', 'í˜ë“¤']
        if any(keyword in combined_text for keyword in mental_keywords):
            return 'ë©˜íƒˆ'
        
        # ê¸°ë³¸ê°’
        return 'ì¼ë°˜'
    
    def _validate_comment(self, comment: str, check_duplicate: bool = True, 
                         post_id: Optional[str] = None) -> Tuple[bool, Optional[ValidationFailureReason]]:
        """
        ëŒ“ê¸€ í’ˆì§ˆ ê²€ì¦ (ìƒˆë¡œìš´ ê·œì¹™ ì ìš©)
        
        Args:
            comment: ê²€ì¦í•  ëŒ“ê¸€
            check_duplicate: ì¤‘ë³µ ì²´í¬ ì—¬ë¶€
            post_id: ê²Œì‹œê¸€ ID (ê²Œì‹œê¸€ë³„ ì¤‘ë³µ ì²´í¬ìš©)
        
        Returns:
            (ê²€ì¦ í†µê³¼ ì—¬ë¶€, ì‹¤íŒ¨ ì›ì¸)
        """
        if not comment:
            return False, ValidationFailureReason.EMPTY
        
        cleaned = comment.strip()
        char_count = len(cleaned.replace(' ', '').replace('\n', ''))
        
        # ì»¤ë®¤ë‹ˆí‹° í† í° ì²´í¬ (ã…‹ã…‹, ã… ã… , ã„·ã„·, ã…ã…, ã…œã…œ ë“±)
        community_tokens = ['ã…‹', 'ã… ', 'ã„·', 'ã…', 'ã…œ', 'ã……', 'ã…‡']
        has_community_token = any(token in cleaned for token in community_tokens)
        
        # 1. ê¸¸ì´ ê²€ì¦ (2~20ìë¡œ ì™„í™”)
        # ë§¤ìš° ì§§ì€ ë°˜ì‘ë„ í—ˆìš©, ìµœëŒ€ ê¸¸ì´ë„ ì™„í™”
        if char_count < 2:
            return False, ValidationFailureReason.TOO_SHORT
        if char_count > 20:  # 14ìì—ì„œ 20ìë¡œ ì™„í™”
            return False, ValidationFailureReason.TOO_LONG
        
        # 2. ì¤„ ìˆ˜ ê²€ì¦
        if '\n' in cleaned:
            return False, ValidationFailureReason.MULTILINE
        
        # 3. ê¸ˆì§€ í‘œí˜„ ê²€ì¦
        cleaned_lower = cleaned.lower()
        for phrase in self.FORBIDDEN_PHRASES:
            if phrase in cleaned_lower:
                return False, ValidationFailureReason.BANNED_WORD
        
        # 4. ë¸”ë™ë¦¬ìŠ¤íŠ¸ ê²€ì¦
        if cleaned in self.blacklist:
            return False, ValidationFailureReason.BLACKLISTED
        
        # 5. ì´ëª¨ì§€ ê²€ì¦ (ì™„í™” - ì‹¤ì œ ì´ëª¨ì§€ë§Œ ì²´í¬)
        # íŠ¹ìˆ˜ë¬¸ìëŠ” ì»¤ë®¤ë‹ˆí‹° ëŒ“ê¸€ì˜ ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ì´ë¯€ë¡œ ê²€ì¦í•˜ì§€ ì•ŠìŒ
        # ì´ëª¨ì§€ íŒ¨í„´ì„ ë” ì—„ê²©í•˜ê²Œ (ì‹¤ì œ ì´ëª¨ì§€ë§Œ)
        emoji_pattern = re.compile(
            "["
            "\U0001F600-\U0001F64F"  # emoticons
            "\U0001F300-\U0001F5FF"  # symbols & pictographs
            "\U0001F680-\U0001F6FF"  # transport & map symbols
            "\U0001F1E0-\U0001F1FF"  # flags
            "]+", flags=re.UNICODE)
        # ì´ëª¨ì§€ê°€ ëª…í™•í•˜ê²Œ í¬í•¨ëœ ê²½ìš°ë§Œ ì‹¤íŒ¨ (í•œ ê¸€ì ì´ìƒ)
        if emoji_pattern.search(cleaned) and len(emoji_pattern.findall(cleaned)) > 0:
            # ì´ëª¨ì§€ê°€ ì „ì²´ ëŒ“ê¸€ì˜ ëŒ€ë¶€ë¶„ì„ ì°¨ì§€í•˜ëŠ” ê²½ìš°ë§Œ ì‹¤íŒ¨
            emoji_chars = emoji_pattern.findall(cleaned)
            total_emoji_length = sum(len(e) for e in emoji_chars)
            if total_emoji_length > len(cleaned) * 0.5:  # ì´ëª¨ì§€ê°€ 50% ì´ìƒ
                return False, ValidationFailureReason.SPECIAL_CHAR_SPAM
        
        # 7. "ê¹”ë”í•¨/ì •ì¤‘í•¨/ì •ë³´ì„±" ê°ì§€ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        formal_words = ['ê°ì‚¬í•©ë‹ˆë‹¤', 'ê°ì‚¬ë“œë¦½ë‹ˆë‹¤', 'ë¶€íƒë“œë¦½ë‹ˆë‹¤', 'ë„ì™€ì£¼ì„¸ìš”', 
                       'ì•Œê² ìŠµë‹ˆë‹¤', 'ì´í•´í–ˆìŠµë‹ˆë‹¤', 'í™•ì¸í–ˆìŠµë‹ˆë‹¤', 'ì°¸ê³ í•˜ê² ìŠµë‹ˆë‹¤']
        if any(word in cleaned for word in formal_words):
            return False, ValidationFailureReason.BANNED_WORD
        
        # 8. ë°˜ë§ ê°ì§€ (ì¡´ëŒ“ë§ë§Œ í—ˆìš©)
        # ë°˜ë§ íŒ¨í„´: ~ì•¼, ~ì§€, ~ë„¤(ë°˜ë§), ~ì–´, ~ì•„ (ë¬¸ì¥ ë)
        # ë‹¨, "~ë„¤ìš”", "~ë„¤ì˜" ê°™ì€ ì¡´ëŒ“ë§ì€ í—ˆìš©
        if not any(word in cleaned for word in ['ë„¤ìš”', 'ë„¤ì˜', 'ë„¤ì—¬', 'ì„¸ìš”', 'ì„¸ì˜', 'ì„¸ì—¬', 'ìš”', 'ì˜', 'ì—¬', 'í•©ë‹ˆ', 'ë“œë¦½ë‹ˆ']):
            banmal_patterns = [
                r'[ê°€-í£]+ì•¼$',  # "ë‚˜ë„ ê³§ í‡´ê·¼ì´ì•¼"
                r'[ê°€-í£]+ì§€$',  # "ê·¸ë ‡ì§€"
                r'[ê°€-í£]+ë„¤$',  # "ê·¸ë ‡ë„¤" (ë°˜ë§)
                r'[ê°€-í£]+ì–´$',  # "ê°€ë´"
                r'[ê°€-í£]+ì•„$',  # "ê°€ë´"
            ]
            for pattern in banmal_patterns:
                if re.search(pattern, cleaned):
                    return False, ValidationFailureReason.BANNED_WORD
        
        # 9. ì„¤ëª…ì /ê°íƒ„ì  í‘œí˜„ ê°ì§€ (ì™„í™”)
        explanatory_words = ['ì§„ì§œ', 'ë„ˆë¬´', 'ì°¸', 'ì •ë§', 'ëŒ€ë‹¨', 'ì™€!', 'ì•„!']
        explanatory_count = sum(1 for word in explanatory_words if word in cleaned)
        # 2ê°œ ì´ìƒì´ë©´ ì‹¤íŒ¨ (ë„ˆë¬´ ì„¤ëª…ì )
        if explanatory_count >= 2:
            return False, ValidationFailureReason.BANNED_WORD
        
        # 10. ì¤‘ë³µ í™•ì¸ (ì˜µì…˜)
        if check_duplicate:
            # ì „ì—­ íˆìŠ¤í† ë¦¬ ì¤‘ë³µ ì²´í¬
            if self._is_duplicate(cleaned):
                return False, ValidationFailureReason.DUPLICATE_RECENT
            
            # ê²Œì‹œê¸€ë³„ ì¤‘ë³µ ì²´í¬
            if post_id and post_id in self.post_comment_map:
                if self.post_comment_map[post_id] == cleaned:
                    return False, ValidationFailureReason.DUPLICATE_POST
        
        return True, None
    
    def _is_duplicate(self, comment: str) -> bool:
        """ìµœê·¼ íˆìŠ¤í† ë¦¬ì™€ ì¤‘ë³µ í™•ì¸ (ìœ ì‚¬ ë¬¸ì¥ë„ ê°ì§€)"""
        cleaned = comment.strip()
        
        # ì™„ì „ ë™ì¼ ì²´í¬
        if cleaned in self.comment_history:
            return True
        
        # ìœ ì‚¬ ë¬¸ì¥ ì²´í¬ (ê³µë°± ì œê±° + ì ‘ë¯¸ì‚¬ ì •ê·œí™”)
        cleaned_normalized = re.sub(r'[ìš”ì—¬ì˜ë‹¹]', 'ìš”', cleaned.replace(' ', ''))
        for hist_comment in self.comment_history[-20:]:  # ìµœê·¼ 20ê°œë§Œ ì²´í¬
            hist_normalized = re.sub(r'[ìš”ì—¬ì˜ë‹¹]', 'ìš”', hist_comment.replace(' ', ''))
            # í•µì‹¬ í‚¤ì›Œë“œê°€ ê°™ê³  ê¸¸ì´ê°€ ë¹„ìŠ·í•˜ë©´ ìœ ì‚¬ë¡œ íŒë‹¨
            if cleaned_normalized == hist_normalized:
                return True
            # í•µì‹¬ í† í° ë¹„êµ (ì¿¨ê±°, ì¡´ê±°ë˜, ë¬´ì‚¬ê·€í™˜ ë“±)
            key_tokens_comment = set(re.findall(r'ì¿¨ê±°|ì¡´ê±°ë˜|ë¬´ì‚¬ê·€í™˜|ë¬´ê·€|ë¬´ì¶œ|ëŒë°œ|ìœ„ì¦ˆ', cleaned))
            key_tokens_hist = set(re.findall(r'ì¿¨ê±°|ì¡´ê±°ë˜|ë¬´ì‚¬ê·€í™˜|ë¬´ê·€|ë¬´ì¶œ|ëŒë°œ|ìœ„ì¦ˆ', hist_comment))
            if key_tokens_comment and key_tokens_comment == key_tokens_hist:
                return True
        
        return False
    
    def _add_to_history(self, comment: str, post_id: Optional[str] = None):
        """íˆìŠ¤í† ë¦¬ì— ì¶”ê°€ (ì „ì—­ + ê²Œì‹œê¸€ë³„)"""
        cleaned = comment.strip()
        if cleaned:
            # ì „ì—­ íˆìŠ¤í† ë¦¬
            self.comment_history.append(cleaned)
            if len(self.comment_history) > self.max_history:
                self.comment_history.pop(0)
            
            # ê²Œì‹œê¸€ë³„ íˆìŠ¤í† ë¦¬
            if post_id:
                self.post_comment_map[post_id] = cleaned
                # ê²Œì‹œê¸€ë³„ ë§µ í¬ê¸° ì œí•œ (ë©”ëª¨ë¦¬ ê´€ë¦¬)
                if len(self.post_comment_map) > 1000:
                    # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±° (FIFO)
                    oldest_key = next(iter(self.post_comment_map))
                    del self.post_comment_map[oldest_key]
    
    def _record_failure(self, reason: ValidationFailureReason):
        """ì‹¤íŒ¨ ì›ì¸ ê¸°ë¡"""
        self.failure_reasons[reason.value] = self.failure_reasons.get(reason.value, 0) + 1
        self.stats['validation_fail_total'] += 1
        self.stats_dirty = True
    
    def _generate_comment_candidates(self, post_content: str, post_title: str = "", 
                                    actual_comments: List[str] = None,
                                    max_retries: int = 2) -> List[str]:
        """OpenAI APIë¡œ ëŒ“ê¸€ í›„ë³´ 8ê°œ ìƒì„± (ì¬ì‹œë„ í¬í•¨)"""
        # API ì œí•œ í™•ì¸
        if self._check_api_limits():
            logger.debug("API ì œí•œ ë„ë‹¬ë¡œ í’€ ëª¨ë“œ ì‚¬ìš©")
            return []
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ (ì œëª©, ë³¸ë¬¸, ëŒ“ê¸€ì—ì„œ)
        extracted_keywords = self._extract_keywords(
            comments=actual_comments,
            post_title=post_title,
            post_content=post_content
        )
        
        # ê±´ìŠ¹ í‚¤ì›Œë“œ ê°ì§€
        combined_text = (post_title + " " + post_content).lower()
        has_geungseung = "ê±´ìŠ¹" in combined_text or "ê±´ìŠ¹" in extracted_keywords
        
        for attempt in range(max_retries):
            try:
                # ìœ ì € ë©”ì‹œì§€ êµ¬ì„±
                user_message = f"ê²Œì‹œê¸€ ì œëª©: {post_title}\nê²Œì‹œê¸€ ë³¸ë¬¸: {post_content}"
                
                # ì¶”ì¶œëœ í‚¤ì›Œë“œ ì¶”ê°€
                if extracted_keywords:
                    user_message += f"\n\nğŸ”‘ ã€ì¤‘ìš” í‚¤ì›Œë“œã€‘\n"
                    user_message += f"{', '.join(extracted_keywords[:8])}\n"
                    user_message += "\nìœ„ í‚¤ì›Œë“œë“¤ì„ ë°˜ë“œì‹œ ì°¸ê³ í•˜ì—¬ ëŒ“ê¸€ì„ ìƒì„±í•˜ì„¸ìš”.\n"
                    user_message += "íŠ¹íˆ 'ê±´ìŠ¹', 'ì¿¨ê±°', 'ë¬´ì‚¬ê·€í™˜', 'ì¡´ê±°ë˜', 'ëŒë°œ', 'ìœ„ì¦ˆ' ê°™ì€ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ í•´ë‹¹ í‚¤ì›Œë“œë¥¼ í¬í•¨í•œ ëŒ“ê¸€ì„ ìš°ì„  ìƒì„±í•˜ì„¸ìš”."
                
                # ê±´ìŠ¹ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ íŠ¹ë³„ ì§€ì‹œ ì¶”ê°€
                if has_geungseung:
                    user_message += "\n\nâš ï¸ ì¤‘ìš”: ì´ ê²Œì‹œê¸€ì— 'ê±´ìŠ¹'ì´ë¼ëŠ” í‚¤ì›Œë“œê°€ ìˆìŠµë‹ˆë‹¤."
                    user_message += "\në°˜ë“œì‹œ 'ê±´ìŠ¹í•˜ì„¸ìš”', 'ê±´ìŠ¹ì…ë‹ˆë‹¤', 'ê±´ìŠ¹í•©ì‹œë‹¤', 'ê±´ìŠ¹ì´ìš”' ê°™ì€ ê±´ìŠ¹ ê´€ë ¨ ëŒ“ê¸€ì„ ìƒì„±í•˜ì„¸ìš”."
                    user_message += "\nê±´ìŠ¹ ê´€ë ¨ í‘œí˜„ì„ í¬í•¨í•œ ëŒ“ê¸€ í›„ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”."
                
                # ì´ë¯¸ ë‹¬ë¦° ëŒ“ê¸€ ì¶”ê°€ (ê°•í™”)
                if actual_comments and len(actual_comments) > 0:
                    filtered_comments = [
                        c for c in actual_comments 
                        if isinstance(c, str) and 2 <= len(c.strip()) <= 20
                    ]
                    if filtered_comments:
                        user_message += f"\n\nã€ì´ë¯¸ ë‹¬ë¦° ì‹¤ì œ ëŒ“ê¸€ë“¤ - ë°˜ë“œì‹œ ì°¸ê³ í•˜ì„¸ìš”ã€‘\n"
                        user_message += "ìœ„ ëŒ“ê¸€ë“¤ì²˜ëŸ¼ ì§§ê³  ë¬´ë‚œí•˜ê²Œ ë°˜ì‘ë§Œ í•˜ì„¸ìš”. ì„¤ëª…í•˜ì§€ ë§ˆì„¸ìš”.\n"
                        for i, comment in enumerate(filtered_comments[:8], 1):  # ìµœëŒ€ 8ê°œ
                            user_message += f"{i}. {comment}\n"
                        user_message += "\nìœ„ ëŒ“ê¸€ë“¤ì˜ í†¤, ê¸¸ì´, ìŠ¤íƒ€ì¼ì„ ì •í™•íˆ ë”°ë¼í•˜ì„¸ìš”.\n"
                        user_message += "- ë°˜ë“œì‹œ ì¡´ëŒ“ë§ ì‚¬ìš© (~ìš”, ~ì˜, ~ì—¬, ~ì„¸ì˜ ë“±)\n"
                        user_message += "- ì„¤ëª…í•˜ì§€ ë§ê³  ì§§ê²Œ ë°˜ì‘ë§Œ (~ì´ì˜, ~ë°”ë¦¬ì˜, ~í•˜ì„¸ìš© ê°™ì€ íŒ¨í„´)\n"
                        user_message += "- 'ì§„ì§œ', 'ë„ˆë¬´', 'ì°¸', 'ì •ë§' ê°™ì€ ì„¤ëª…ì  í‘œí˜„ ìµœì†Œí™”\n"
                
                # API í˜¸ì¶œ
                response = self.client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {"role": "system", "content": self.system_prompt},
                        {"role": "user", "content": user_message}
                    ],
                    temperature=0.8,  # ë‹¤ì–‘ì„±ì„ ìœ„í•´ ë†’ì€ temperature
                    top_p=0.9,
                    max_tokens=120,  # í›„ë³´ 8ê°œ (ê° 4~12ì)
                )
                
                # ì‚¬ìš©ëŸ‰ ì¶”ì 
                self.api_usage['calls_today'] += 1
                if hasattr(response, 'usage'):
                    tokens = response.usage.total_tokens if response.usage else 0
                    self.api_usage['tokens_today'] += tokens
                
                response_text = response.choices[0].message.content.strip()
                
                # í›„ë³´ ëŒ“ê¸€ íŒŒì‹± (ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„)
                candidates = []
                for line in response_text.split('\n'):
                    line = line.strip()
                    # ë²ˆí˜¸ë‚˜ ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
                    line = re.sub(r'^\d+[\.\)]\s*', '', line)  # "1. " ë˜ëŠ” "1) " ì œê±°
                    line = line.strip('"\'')  # ë”°ì˜´í‘œ ì œê±°
                    if line and len(line.replace(' ', '')) >= 2:  # ìµœì†Œ ê¸¸ì´ ì²´í¬ (ê³µë°± ì œì™¸, 2ì ì´ìƒ)
                        candidates.append(line)
                
                # ìƒì„±ëœ í›„ë³´ ë¡œê¹… (ë””ë²„ê¹…ìš©)
                if candidates:
                    logger.debug(f"ìƒì„±ëœ í›„ë³´ ëª©ë¡: {candidates}")
                
                if candidates:
                    logger.debug(f"ëŒ“ê¸€ í›„ë³´ ìƒì„± ì„±ê³µ: {len(candidates)}ê°œ")
                    self.stats['classification_used'] = self.stats.get('classification_used', 0) + 1
                    self.stats_dirty = True
                    return candidates[:8]  # ìµœëŒ€ 8ê°œë§Œ ë°˜í™˜
                else:
                    logger.warning("ìƒì„±ëœ í›„ë³´ê°€ ì—†ìŒ")
                    if attempt < max_retries - 1:
                        continue
                    else:
                        return []
                    
            except RateLimitError as e:
                logger.warning(f"Rate limit ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                self.stats['api_errors'] += 1
                self.stats_dirty = True
                if attempt < max_retries - 1:
                    wait_time = 5 * (2 ** attempt)
                    time.sleep(wait_time)
                    continue
                else:
                    return []
            except APIConnectionError as e:
                logger.warning(f"ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                self.stats['api_errors'] += 1
                self.stats_dirty = True
                if attempt < max_retries - 1:
                    wait_time = 1 * (3 ** attempt)
                    time.sleep(wait_time)
                    continue
                else:
                    return []
            except APIError as e:
                logger.error(f"API ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                self.stats['api_errors'] += 1
                self.stats_dirty = True
                if attempt < max_retries - 1:
                    time.sleep(0.5 * (attempt + 1))
                    continue
                else:
                    return []
            except Exception as e:
                logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                self.stats['api_errors'] += 1
                self.stats_dirty = True
                if attempt < max_retries - 1:
                    time.sleep(0.5 * (attempt + 1))
                    continue
                else:
                    return []
        
        return []
    
    def _get_from_pool(self, post_type: str = 'ì¼ë°˜', 
                      exclude_comments: List[str] = None, 
                      post_id: Optional[str] = None) -> Optional[str]:
        """ëŒ“ê¸€ í’€ì—ì„œ ì„ íƒ (ìœ í˜•ë³„, ì¤‘ë³µ ì œì™¸, ë°˜ë³µ ë°©ì§€ ê°•í™”)"""
        # ìœ í˜•ë³„ í’€ ì„ íƒ
        if post_type not in self.comment_pool:
            post_type = 'ì¼ë°˜'
        
        type_pool = self.comment_pool.get(post_type, self.comment_pool.get('ì¼ë°˜', []))
        
        if not type_pool:
            # í•´ë‹¹ ìœ í˜• í’€ì´ ë¹„ì–´ìˆìœ¼ë©´ ì¼ë°˜ í’€ ì‚¬ìš©
            type_pool = self.comment_pool.get('ì¼ë°˜', [])
        
        exclude_set = set(exclude_comments or [])
        exclude_set.update(self.comment_history)
        exclude_set.update(self.blacklist)
        
        # ê²Œì‹œê¸€ë³„ íˆìŠ¤í† ë¦¬ë„ ì œì™¸
        if post_id and post_id in self.post_comment_map:
            exclude_set.add(self.post_comment_map[post_id])
        
        # ì¤‘ë³µ ì²´í¬ (ìœ ì‚¬ ë¬¸ì¥ í¬í•¨)
        available = []
        for c in type_pool:
            if c not in exclude_set and not self._is_duplicate(c):
                available.append(c)
        
        if available:
            comment = random.choice(available)
            self.stats['pool_used'] += 1
            self.stats['generated_total'] += 1
            self.stats_dirty = True
            self._save_stats()
            return comment
        
        # í’€ì— ì‚¬ìš© ê°€ëŠ¥í•œ ëŒ“ê¸€ì´ ì—†ìœ¼ë©´ íˆìŠ¤í† ë¦¬ ì¼ë¶€ë§Œ ë¬´ì‹œ (ìµœê·¼ 10ê°œë§Œ)
        recent_history = self.comment_history[-10:] if len(self.comment_history) > 10 else []
        exclude_set = set(exclude_comments or [])
        exclude_set.update(recent_history)
        exclude_set.update(self.blacklist)
        
        if post_id and post_id in self.post_comment_map:
            exclude_set.add(self.post_comment_map[post_id])
        
        available = []
        for c in type_pool:
            if c not in exclude_set:
                # ìœ ì‚¬ë„ ì²´í¬ëŠ” ì™„í™” (ìµœê·¼ íˆìŠ¤í† ë¦¬ë§Œ ì²´í¬)
                is_dup = False
                for hist in recent_history:
                    if c == hist:
                        is_dup = True
                        break
                if not is_dup:
                    available.append(c)
        
        if available:
            comment = random.choice(available)
            self.stats['pool_used'] += 1
            self.stats['generated_total'] += 1
            self.stats_dirty = True
            self._save_stats()
            logger.warning(f"ëŒ“ê¸€ í’€ ì„ íƒ: ìµœê·¼ íˆìŠ¤í† ë¦¬ ì¼ë¶€ ë¬´ì‹œ (ì‚¬ìš© ê°€ëŠ¥í•œ ëŒ“ê¸€ ë¶€ì¡±)")
            return comment
        
        # ê·¸ë˜ë„ ì—†ìœ¼ë©´ ë¸”ë™ë¦¬ìŠ¤íŠ¸ë§Œ ì œì™¸í•˜ê³  ì„ íƒ (ìµœí›„ì˜ ìˆ˜ë‹¨)
        available = [c for c in type_pool if c not in self.blacklist]
        if available:
            comment = random.choice(available)
            self.stats['pool_used'] += 1
            self.stats['generated_total'] += 1
            self.stats_dirty = True
            self._save_stats()
            logger.warning(f"ëŒ“ê¸€ í’€ ì„ íƒ: íˆìŠ¤í† ë¦¬ ë¬´ì‹œ (ì‚¬ìš© ê°€ëŠ¥í•œ ëŒ“ê¸€ ë¶€ì¡±)")
            return comment
        
        return None
    
    def generate_comment_candidates_only(self, post_content: str, post_title: str = "", 
                                         actual_comments: List[str] = None) -> List[str]:
        """
        ëŒ“ê¸€ í›„ë³´ë§Œ ìƒì„± (GUIì—ì„œ ì„ íƒìš©)
        
        Args:
            post_content: ê²Œì‹œê¸€ ë³¸ë¬¸
            post_title: ê²Œì‹œê¸€ ì œëª©
            actual_comments: ì‹¤ì œ ëŒ“ê¸€ ëª©ë¡
        
        Returns:
            ëŒ“ê¸€ í›„ë³´ ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ 8ê°œ)
        """
        if not self.can_generate_comment(post_content):
            return []
        
        # AIë¡œ ëŒ“ê¸€ í›„ë³´ 8ê°œ ìƒì„±
        candidates = []
        if not self.force_pool_mode:
            candidates = self._generate_comment_candidates(
                post_content, post_title, actual_comments
            )
            logger.debug(f"ìƒì„±ëœ ëŒ“ê¸€ í›„ë³´: {len(candidates)}ê°œ")
        
        # ê²€ì¦ í†µê³¼í•œ í›„ë³´ë§Œ ë°˜í™˜
        valid_candidates = []
        for candidate in candidates:
            is_valid, _ = self._validate_comment(candidate, check_duplicate=False)
            if is_valid:
                valid_candidates.append(candidate)
        
        return valid_candidates[:8]
    
    def generate_comment(self, post_content: str, post_title: str = "", 
                        actual_comments: List[str] = None,
                        post_id: Optional[str] = None) -> Optional[str]:
        """
        ëŒ“ê¸€ ìƒì„± (ë©”ì¸ ë©”ì„œë“œ)
        - AIê°€ ëŒ“ê¸€ í›„ë³´ 8ê°œ ìƒì„±
        - í›„ë³´ ì¤‘ì—ì„œ ê²€ì¦ í†µê³¼í•œ ê²ƒ ì¤‘ í•˜ë‚˜ ì„ íƒ
        
        Args:
            post_content: ê²Œì‹œê¸€ ë³¸ë¬¸
            post_title: ê²Œì‹œê¸€ ì œëª©
            actual_comments: ì‹¤ì œ ëŒ“ê¸€ ëª©ë¡ (AIì—ê²Œ ì „ë‹¬í•˜ì—¬ ì°¸ê³ )
            post_id: ê²Œì‹œê¸€ ID (ê²Œì‹œê¸€ë³„ ì¤‘ë³µ ë°©ì§€ìš©)
        
        Returns:
            ìƒì„±ëœ ëŒ“ê¸€ ë˜ëŠ” None
        """
        # ì£¼ê¸°ì  í•«ë¦¬ë¡œë“œ ì²´í¬
        current_time = time.time()
        if current_time - self.last_pool_reload >= self.hot_reload_interval:
            self.reload_comment_pool()
            self.last_pool_reload = current_time
        
        # ì£¼ê¸°ì  í†µê³„ ì €ì¥
        self._save_stats()
        
        if not self.can_generate_comment(post_content):
            self.stats['skipped'] += 1
            self.stats_dirty = True
            self._save_stats()
            return None
        
        # ê±´ìŠ¹ í‚¤ì›Œë“œ ê°ì§€
        combined_text = (post_title + " " + post_content).lower()
        has_geungseung = "ê±´ìŠ¹" in combined_text
        
        # 1ë‹¨ê³„: AIë¡œ ëŒ“ê¸€ í›„ë³´ 8ê°œ ìƒì„±
        candidates = []
        if not self.force_pool_mode:
            candidates = self._generate_comment_candidates(
                post_content, post_title, actual_comments
            )
            logger.debug(f"ìƒì„±ëœ ëŒ“ê¸€ í›„ë³´: {len(candidates)}ê°œ")
        
        # ê±´ìŠ¹ í‚¤ì›Œë“œê°€ ìˆê³  í›„ë³´ê°€ ì—†ìœ¼ë©´ ê±´ìŠ¹ í’€ì—ì„œ ìš°ì„  ì„ íƒ
        if has_geungseung and not candidates:
            logger.debug("ê±´ìŠ¹ í‚¤ì›Œë“œ ê°ì§€: ê±´ìŠ¹ í’€ì—ì„œ ìš°ì„  ì„ íƒ")
            geungseung_comment = self._get_from_pool(
                post_type='ê±´ìŠ¹',
                exclude_comments=[post_content] if post_content else None,
                post_id=post_id
            )
            if geungseung_comment:
                is_valid, failure_reason = self._validate_comment(geungseung_comment, check_duplicate=True, post_id=post_id)
                if is_valid:
                    self._add_to_history(geungseung_comment, post_id)
                    self.stats['pool_used'] += 1
                    self.stats['generated_total'] += 1
                    self.stats_dirty = True
                    self._save_stats()
                    return geungseung_comment
        
        # 2ë‹¨ê³„: í›„ë³´ ì¤‘ì—ì„œ ê²€ì¦ í†µê³¼í•œ ê²ƒ í•„í„°ë§
        valid_candidates = []
        for candidate in candidates:
            is_valid, failure_reason = self._validate_comment(
                candidate, check_duplicate=True, post_id=post_id
            )
            if is_valid:
                valid_candidates.append(candidate)
            else:
                if failure_reason:
                    logger.warning(f"í›„ë³´ ê²€ì¦ ì‹¤íŒ¨: '{candidate}' (ê¸¸ì´: {len(candidate.replace(' ', ''))}ì) - {failure_reason.value}")
        
        # 3ë‹¨ê³„: ê²€ì¦ í†µê³¼í•œ í›„ë³´ ì¤‘ì—ì„œ í•˜ë‚˜ ì„ íƒ
        if valid_candidates:
            # ì¤‘ë³µ ì²´í¬ë¥¼ ë‹¤ì‹œ í•œ ë²ˆ ìˆ˜í–‰ (íˆìŠ¤í† ë¦¬ì™€ ë¹„êµ)
            final_candidates = []
            for candidate in valid_candidates:
                if not self._is_duplicate(candidate):
                    final_candidates.append(candidate)
            
            if final_candidates:
                # ê±´ìŠ¹ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ê±´ìŠ¹ ê´€ë ¨ ëŒ“ê¸€ ìš°ì„  ì„ íƒ
                if has_geungseung:
                    geungseung_candidates = [c for c in final_candidates if 'ê±´ìŠ¹' in c]
                    if geungseung_candidates:
                        comment = random.choice(geungseung_candidates)
                        logger.debug(f"ê±´ìŠ¹ ê´€ë ¨ ëŒ“ê¸€ ì„ íƒ: {comment}")
                    else:
                        comment = random.choice(final_candidates)
                else:
                    comment = random.choice(final_candidates)
                
                self._add_to_history(comment, post_id)
                self.stats['gpt_used'] += 1
                self.stats['generated_total'] += 1
                self.stats_dirty = True
                self._save_stats()
                logger.debug(f"ìµœì¢… ì„ íƒëœ ëŒ“ê¸€: {comment}")
                return comment
        
        # 4ë‹¨ê³„: AI ìƒì„± ì‹¤íŒ¨ ì‹œ í’€ì—ì„œ ì„ íƒ (fallback)
        logger.debug("AI ìƒì„± ì‹¤íŒ¨, í’€ ëª¨ë“œë¡œ ì „í™˜")
        
        # ê±´ìŠ¹ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ê±´ìŠ¹ í’€ ìš°ì„  ì‚¬ìš©
        if has_geungseung:
            logger.debug("ê±´ìŠ¹ í‚¤ì›Œë“œ ê°ì§€: ê±´ìŠ¹ í’€ ìš°ì„  ì‚¬ìš©")
            comment = self._get_from_pool(
                post_type='ê±´ìŠ¹',
                exclude_comments=[post_content] if post_content else None,
                post_id=post_id
            )
            if comment:
                is_valid, failure_reason = self._validate_comment(comment, check_duplicate=True, post_id=post_id)
                if is_valid:
                    self._add_to_history(comment, post_id)
                    return comment
        
        # íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ìœ í˜• íŒë‹¨
        fallback_type = self._detect_post_type_heuristic(post_content, post_title)
        logger.debug(f"Fallback ìœ í˜• íŒë‹¨: {fallback_type}")
        comment = self._get_from_pool(
            post_type=fallback_type,
            exclude_comments=[post_content] if post_content else None,
            post_id=post_id
        )
        
        if comment:
            is_valid, failure_reason = self._validate_comment(comment, check_duplicate=True, post_id=post_id)
            if is_valid:
                self._add_to_history(comment, post_id)
                return comment
            else:
                if failure_reason:
                    self._record_failure(failure_reason)
                    logger.warning(f"í’€ì—ì„œ ê°€ì ¸ì˜¨ ëŒ“ê¸€ ê²€ì¦ ì‹¤íŒ¨: {comment} - {failure_reason.value}")
        
        # ëª¨ë“  ë°©ë²• ì‹¤íŒ¨
        self.stats['skipped'] += 1
        self.stats_dirty = True
        self._save_stats(force=True)
        logger.warning("ëŒ“ê¸€ ìƒì„± ì‹¤íŒ¨: ëª¨ë“  ë°©ë²• ì‹œë„ ì™„ë£Œ")
        return None
    
    def add_to_blacklist(self, comment: str):
        """ë¸”ë™ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€"""
        self.blacklist.add(comment.strip())
        self._save_comment_pool()
        logger.info(f"ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì¶”ê°€: {comment}")
    
    def get_stats(self) -> Dict:
        """í†µê³„ ì •ë³´ ë°˜í™˜"""
        stats = self.stats.copy()
        stats['failure_reasons'] = self.failure_reasons.copy()
        stats['api_usage'] = self.api_usage.copy()
        stats['force_pool_mode'] = self.force_pool_mode
        return stats
    
    def reset_history(self):
        """íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.comment_history.clear()
        self.post_comment_map.clear()
        logger.info("ëŒ“ê¸€ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”")
    
    def save_stats_now(self):
        """í†µê³„ ì¦‰ì‹œ ì €ì¥ (í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì‹œ í˜¸ì¶œ)"""
        self._save_stats(force=True)
    
    def _load_likes(self):
        """ì¢‹ì•„ìš” ë°ì´í„° ë¡œë“œ"""
        try:
            if os.path.exists(self.likes_file):
                with open(self.likes_file, 'r', encoding='utf-8') as f:
                    # íŒŒì¼ ë½
                    try:
                        if os.name == 'nt':
                            try:
                                msvcrt.locking(f.fileno(), msvcrt.LK_LOCK, 1)
                            except NameError:
                                pass
                        else:
                            try:
                                fcntl.flock(f.fileno(), fcntl.LOCK_SH)
                            except NameError:
                                pass
                    except:
                        pass
                    
                    data = json.load(f)
                    self.likes = {k: v for k, v in data.items() if v is True}
                    
                    # ë½ í•´ì œ
                    try:
                        if os.name == 'nt':
                            try:
                                msvcrt.locking(f.fileno(), msvcrt.LK_UNLCK, 1)
                            except NameError:
                                pass
                        else:
                            try:
                                fcntl.flock(f.fileno(), fcntl.LOCK_UN)
                            except NameError:
                                pass
                    except:
                        pass
                    
                    logger.info(f"ì¢‹ì•„ìš” ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.likes)}ê°œ")
            else:
                self.likes = {}
                logger.info("ì¢‹ì•„ìš” ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"ì¢‹ì•„ìš” ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
            self.likes = {}
    
    def _save_likes(self):
        """ì¢‹ì•„ìš” ë°ì´í„° ì €ì¥"""
        try:
            temp_file = self.likes_file + '.tmp'
            with open(temp_file, 'w', encoding='utf-8') as f:
                # íŒŒì¼ ë½
                try:
                    if os.name == 'nt':
                        try:
                            msvcrt.locking(f.fileno(), msvcrt.LK_LOCK, 1)
                        except NameError:
                            pass
                    else:
                        try:
                            fcntl.flock(f.fileno(), fcntl.LOCK_EX)
                        except NameError:
                            pass
                except:
                    pass
                
                json.dump(self.likes, f, ensure_ascii=False, indent=2)
                f.flush()
                os.fsync(f.fileno())
                
                # ë½ í•´ì œ
                try:
                    if os.name == 'nt':
                        try:
                            msvcrt.locking(f.fileno(), msvcrt.LK_UNLCK, 1)
                        except NameError:
                            pass
                    else:
                        try:
                            fcntl.flock(f.fileno(), fcntl.LOCK_UN)
                        except NameError:
                            pass
                except:
                    pass
            
            # ì›ìì  ì´ë™
            if os.path.exists(self.likes_file):
                os.replace(temp_file, self.likes_file)
            else:
                os.rename(temp_file, self.likes_file)
            
            logger.debug("ì¢‹ì•„ìš” ë°ì´í„° ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ì¢‹ì•„ìš” ë°ì´í„° ì €ì¥ ì˜¤ë¥˜: {e}")
            try:
                if os.path.exists(temp_file):
                    os.remove(temp_file)
            except:
                pass
    
    def toggle_like(self, post_id: str) -> bool:
        """
        ì¢‹ì•„ìš” í† ê¸€ (ëˆ„ë¥´ê¸°/ì·¨ì†Œ)
        
        Args:
            post_id: ê²Œì‹œê¸€ ID
        
        Returns:
            ì¢‹ì•„ìš” ìƒíƒœ (True: ì¢‹ì•„ìš” ëˆ„ë¦„, False: ì¢‹ì•„ìš” ì·¨ì†Œ)
        """
        if not post_id:
            return False
        
        if post_id in self.likes and self.likes[post_id]:
            # ì¢‹ì•„ìš” ì·¨ì†Œ
            del self.likes[post_id]
            self._save_likes()
            logger.info(f"ì¢‹ì•„ìš” ì·¨ì†Œ: {post_id}")
            return False
        else:
            # ì¢‹ì•„ìš” ëˆ„ë¥´ê¸°
            self.likes[post_id] = True
            self._save_likes()
            logger.info(f"ì¢‹ì•„ìš” ëˆ„ë¦„: {post_id}")
            return True
    
    def is_liked(self, post_id: str) -> bool:
        """
        ì¢‹ì•„ìš” ìƒíƒœ í™•ì¸
        
        Args:
            post_id: ê²Œì‹œê¸€ ID
        
        Returns:
            ì¢‹ì•„ìš” ìƒíƒœ (True: ì¢‹ì•„ìš” ëˆ„ë¦„, False: ì¢‹ì•„ìš” ì•ˆ ëˆ„ë¦„)
        """
        if not post_id:
            return False
        return self.likes.get(post_id, False)
    
    def get_likes_count(self) -> int:
        """ì „ì²´ ì¢‹ì•„ìš” ê°œìˆ˜ ë°˜í™˜"""
        return len(self.likes)
